{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1dde045",
   "metadata": {},
   "source": [
    "# Before Start\n",
    "### install packages numpy pandas scipy scikit-learn statsmodels geomstats matplotlib seaborn by\n",
    "> !pip install numpy pandas scipy scikit-learn statsmodels geomstats matplotlib seaborn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "945d74e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(os.path.join(*str(os.getcwd()).split('\\\\')[:-1]).replace(':',':\\\\'))\n",
    "import riemannian_robust_m_estimator as rrm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import os, time, pickle\n",
    "import math\n",
    "from scipy.stats import chi2\n",
    "\n",
    "import geomstats.backend as gs\n",
    "import geomstats.visualization as visualization\n",
    "from geomstats.geometry.euclidean import Euclidean\n",
    "from geomstats.geometry.hyperboloid import Hyperboloid\n",
    "from geomstats.geometry.hypersphere import Hypersphere\n",
    "from geomstats.geometry.poincare_ball import PoincareBall\n",
    "from geomstats.geometry.poincare_half_space import PoincareHalfSpace\n",
    "from geomstats.geometry.spd_matrices import SPDMatrices\n",
    "from geomstats.geometry.special_euclidean import SpecialEuclidean\n",
    "from geomstats.geometry.special_orthogonal import SpecialOrthogonal\n",
    "from geomstats.learning.frechet_mean import FrechetMean\n",
    "from geomstats.learning.geometric_median import GeometricMedian\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "489df9c4-b4fa-495b-bde4-51246f1f3827",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "logging.getLogger('root').setLevel(logging.ERROR)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b816e9",
   "metadata": {},
   "source": [
    "# Sample mean simulation code\n",
    "1) Set $\\mu=(0,0,...,1)$ and covariance matrix $\\Sigma$ which is the same as hyperbolic(hyperboloid)/euclidean\n",
    "2) Generate random samples $\\mathbf{X} \\in \\mathbb{R}^{n \\times d}$ from pre-specified distribution with $\\mu$, covariance matrix $\\Sigma$\n",
    "3) Send to the tangent space of $\\mu=(0,0,...,1)$ by adding 1 to the last axis and denote this as $\\mathbf{X}'$.\n",
    "4) Get $\\mathbf{X}^* = Exp_{\\mu}(\\mathbf{X}')$ for manifold-valued sample, $Exp$ is the exponential map of the sphere manifold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42d38d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def riemannian_sample_variance_comparison(manifold, mu, cov, n_samples, pop_dist_type = 'gaussian', sample_iter=100, seed=None):\n",
    "\n",
    "    max_iter_graddescent = 2048\n",
    "    epsilon = 1e-7\n",
    "    cutoff_ptiles = [5,10,15,20,25,30,35,40,45,50,60,70,75,80,82.13,85,90,95,99,99.73]\n",
    "    results = {\n",
    "        'mean':{'fm':[],'gm':[],\n",
    "               },\n",
    "        'var':{'fm':[],'gm':[],\n",
    "               },\n",
    "        'sq_bias':{'fm':[],'gm':[],\n",
    "               },\n",
    "        'sample_mean_var':{},\n",
    "        'sample_var_mean':{},\n",
    "        'sample_var_var':{},\n",
    "        'sq_bias_mean':{},\n",
    "        'sq_bias_var':{},\n",
    "        'invalid':[],\n",
    "        'cov':cov,\n",
    "        'distances':[],\n",
    "        'cutoff':{}\n",
    "    }\n",
    "    if seed is not None:\n",
    "        gs.random.seed(seed)\n",
    "        my_rng = np.random.default_rng(seed=seed)\n",
    "    else:\n",
    "        my_rng = np.random.default_rng()\n",
    "\n",
    "    for cutoff_pct in cutoff_ptiles:\n",
    "        results['cutoff'][cutoff_pct] = []\n",
    "    m_est_type_lb = {'mono':('huber',1.345),'softr':('cauchy',2.3849),'hardr':('hampel',1.35)}  #'hardr':('biweight',4.6851)\n",
    "    \n",
    "    for i in range(sample_iter):\n",
    "        if pop_dist_type == 'gaussian':\n",
    "            tangent_data = np.random.multivariate_normal(mean=mu[:-1], cov=cov, size=n_samples)\n",
    "        elif pop_dist_type == 't':\n",
    "            tangent_data = sample_mv_t(mean=mu[:-1], Sigma=cov, n=n_samples, nu=4, rng=my_rng)\n",
    "        elif pop_dist_type == 'laplace':\n",
    "            tangent_data = sample_mv_laplace(mean=mu[:-1], Sigma=cov, n=n_samples, rng=my_rng)\n",
    "        elif pop_dist_type == 'NIG':\n",
    "            tangent_data = sample_mv_nig(mean=mu[:-1], Sigma=cov, n=n_samples, alpha=1.5, delta=1.0, rng=my_rng)\n",
    "\n",
    "       \n",
    "        tangent_data = np.hstack((tangent_data, np.expand_dims(np.repeat(0,n_samples), -1)))\n",
    "        logs = mu + tangent_data\n",
    "        data = manifold.metric.exp(logs, mu)\n",
    "       \n",
    "        dists = manifold.metric.dist(data, mu)\n",
    "        results['distances'].append(dists)\n",
    "        if i==0:\n",
    "            print(f'distances distribution(avg:{np.mean(dists):.2f}/max:{np.max(dists):.2f}/min:{np.min(dists):.2f})')\n",
    "\n",
    "        fm = rrm.RiemannianRobustMestimator(\n",
    "                    space=manifold,\n",
    "                    method='default',\n",
    "                    m_estimator='custom',\n",
    "                    critical_value=None,\n",
    "                    init_point_method='mean-projection',\n",
    "                )\n",
    "        fm.set_loss(squaredist)\n",
    "        fm.set(init_step_size=0.5, max_iter=max_iter_graddescent, epsilon=epsilon)\n",
    "        fm.fit(data)\n",
    "                \n",
    "        if len(fm.estimate_.losses)>max_iter_graddescent-50:\n",
    "            results['invalid'].append(f'{res_lb}_{i}')\n",
    "            results['mean'][res_lb].append(gs.array([9999]*len(data[0])))\n",
    "            results['var'][res_lb].append(9999)\n",
    "            results['sq_bias'][res_lb].append(9999)\n",
    "        else:\n",
    "            results['mean']['fm'].append(fm.estimate_.x)\n",
    "            results['var']['fm'].append(\n",
    "                rrm.riemannian_variance(manifold, data, base=fm.estimate_.x)\n",
    "            )\n",
    "            bias = fm.estimate_.x - mu\n",
    "            results['sq_bias']['fm'].append(\n",
    "                bias.T @ bias\n",
    "            )\n",
    "   \n",
    "        gm = GeometricMedian(manifold, lr=0.5, max_iter=max_iter_graddescent, epsilon=epsilon)\n",
    "        gm.fit(data)\n",
    "        results['mean']['gm'].append(gm.estimate_)\n",
    "        results['var']['gm'].append(\n",
    "            rrm.riemannian_variance(manifold, data, base=gm.estimate_)\n",
    "        )\n",
    "        bias = gm.estimate_ - mu\n",
    "        results['sq_bias']['gm'].append(\n",
    "            bias.T @ bias\n",
    "        )\n",
    "       \n",
    "        for cutoff_pct in cutoff_ptiles:\n",
    "            ptile_ix = int(len(dists)*cutoff_pct/100)-1\n",
    "            c = np.sort(dists)[ptile_ix]\n",
    "            results['cutoff'][cutoff_pct].append(c)\n",
    "            for m_est_tp,(m_est_lb,c_95) in m_est_type_lb.items():\n",
    "                res_lb = f'{m_est_tp}_{cutoff_pct:.2f}'\n",
    "                if res_lb not in results['mean'].keys():\n",
    "                    results['mean'][res_lb] = []\n",
    "                    results['var'][res_lb] = []\n",
    "                    results['sq_bias'][res_lb] = []\n",
    "                    \n",
    "                # print(f'{res_lb}({c:.2f}) start')\n",
    "                m_est = rrm.RiemannianRobustMestimator(\n",
    "                    space=manifold, method='default', m_estimator=m_est_lb, critical_value=c,init_point_method='mean-projection',\n",
    "                )\n",
    "                m_est.set(init_step_size=5 if m_est_lb == 'biweight' else 0.5, max_iter=max_iter_graddescent, epsilon=epsilon)\n",
    "                m_est.fit(data)\n",
    "                if len(m_est.estimate_.losses)>max_iter_graddescent-50:\n",
    "                    results['invalid'].append(f'{res_lb}_{i}')\n",
    "                    results['mean'][res_lb].append(gs.array([9999]*len(data[0])))\n",
    "                    results['var'][res_lb].append(9999)\n",
    "                    results['sq_bias'][res_lb].append(9999)\n",
    "                else:\n",
    "                    results['mean'][res_lb].append(m_est.estimate_.x)\n",
    "                    results['var'][res_lb].append(\n",
    "                        rrm.riemannian_variance(manifold, data, base=m_est.estimate_.x)\n",
    "                    )\n",
    "                    bias = m_est.estimate_.x - mu\n",
    "                    results['sq_bias'][res_lb].append(\n",
    "                        bias.T @ bias\n",
    "                    )\n",
    "               \n",
    "\n",
    "    for lb in results['mean'].keys():\n",
    "        results['sample_mean_var'][lb] = rrm.riemannian_variance(manifold, gs.array([i for i in results['mean'][lb] if i[0]!=9999])),\n",
    "        results['sample_var_mean'][lb] = np.mean([i for i in results['var'][lb] if i!=9999]),\n",
    "        results['sample_var_var'][lb] = np.var([i for i in results['var'][lb] if i!=9999]),\n",
    "        results['sq_bias_mean'][lb] = np.mean([i for i in results['sq_bias'][lb] if i!=9999]),\n",
    "        results['sq_bias_var'][lb] = np.var([i for i in results['sq_bias'][lb] if i!=9999]),\n",
    "       \n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "# --------- 유틸 ---------\n",
    "def chol_psd_for_cov(Sigma, scale=1.0, eps=1e-12):\n",
    "    \"\"\"L L^T = Sigma / scale 가 되도록 수치안정 고유분해 기반 'Cholesky-like'.\"\"\"\n",
    "    Sigma = np.asarray(Sigma, float)\n",
    "    vals, vecs = np.linalg.eigh(Sigma)\n",
    "    vals = np.clip(vals, eps, None)\n",
    "    target = vals / float(scale)\n",
    "    return vecs @ np.diag(np.sqrt(target)) @ vecs.T\n",
    "\n",
    "# Inverse-Gaussian(μ, λ) 샘플러 (Michael–Schucany–Haas)\n",
    "def sample_inverse_gaussian(mu, lam, size, rng):\n",
    "    v = rng.standard_normal(size=size)\n",
    "    y = v**2\n",
    "    x = mu + (mu**2 * y)/(2*lam) - (mu/(2*lam))*np.sqrt(4*mu*lam*y + mu**2 * y**2)\n",
    "    u = rng.uniform(size=size)\n",
    "    w = np.where(u <= mu/(mu + x), x, (mu**2)/x)\n",
    "    return w\n",
    "\n",
    "# --------- 1) Student-t_ν (ν>2) ---------\n",
    "def sample_mv_t(mean, Sigma, n, nu, rng=None):\n",
    "    \"\"\"\n",
    "    mean: (d,), Sigma: (d,d), nu>2\n",
    "    공분산이 정확히 Sigma가 되도록 자동 스케일.\n",
    "    \"\"\"\n",
    "    assert nu > 2, \"Student-t에서 공분산 유한/매칭은 ν>2 필요.\"\n",
    "    rng = np.random.default_rng() if rng is None else rng\n",
    "    E_W = nu / (nu - 2.0)\n",
    "    L = chol_psd_for_cov(Sigma, scale=E_W)\n",
    "    G = rng.chisquare(df=nu, size=n)          # χ²_ν\n",
    "    W = nu / G                                # mixture\n",
    "    Z = rng.standard_normal((n, mean.size))   # N(0, I)\n",
    "    X = mean + (np.sqrt(W)[:, None] * (Z @ L.T))\n",
    "    return X\n",
    "\n",
    "# --------- 2) Elliptical Laplace ---------\n",
    "def sample_mv_laplace(mean, Sigma, n, rng=None):\n",
    "    \"\"\"\n",
    "    타원형 Laplace: X|W ~ N(mean, W Sigma), W ~ Exp(1), E[W]=1\n",
    "    -> L L^T = Sigma\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng() if rng is None else rng\n",
    "    L = chol_psd_for_cov(Sigma, scale=1.0)\n",
    "    W = rng.exponential(scale=1.0, size=n)\n",
    "    Z = rng.standard_normal((n, mean.size))\n",
    "    X = mean + (np.sqrt(W)[:, None] * (Z @ L.T))\n",
    "    return X\n",
    "\n",
    "# --------- 3) NIG (대칭형, β=0) ---------\n",
    "def sample_mv_nig(mean, Sigma, n, alpha=1.5, delta=1.0, rng=None):\n",
    "    \"\"\"\n",
    "    대칭 NIG: W ~ IG(μ=δ/α, λ=δα), E[W]=δ/α\n",
    "    -> L L^T = Sigma / (δ/α)\n",
    "    (왜도 β!=0 도 가능하지만, 그 경우 Cov에 Var(W)ββ^T가 추가되므로 별도 스케일 조정 필요)\n",
    "    \"\"\"\n",
    "    assert alpha > 0 and delta > 0\n",
    "    rng = np.random.default_rng() if rng is None else rng\n",
    "    muW = delta / alpha\n",
    "    lamW = delta * alpha\n",
    "    L = chol_psd_for_cov(Sigma, scale=muW)\n",
    "    W = sample_inverse_gaussian(mu=muW, lam=lamW, size=n, rng=rng)\n",
    "    Z = rng.standard_normal((n, mean.size))\n",
    "    X = mean + (np.sqrt(W)[:, None] * (Z @ L.T))\n",
    "    return X\n",
    "\n",
    "def squaredist(space,points,base,critical_value,weights=None,loss_and_grad=True):\n",
    "    n = len(points)\n",
    "    logs = space.metric.log(points,base)\n",
    "    dists = space.metric.dist(points,base)\n",
    "\n",
    "    loss = gs.sum(dists**2)/n\n",
    "    if not loss_and_grad:\n",
    "        return loss\n",
    "    grad = -2*gs.sum(logs,axis=0)/n\n",
    "    return loss, space.to_tangent(grad,base)\n",
    "\n",
    "    \n",
    "# # --------- 사용 예시 ---------\n",
    "# if __name__ == \"__main__\":\n",
    "#     d, n = 3, 10000\n",
    "#     mu = np.zeros(d)\n",
    "#     Sigma = np.array([[1.0, 0.6, 0.0],\n",
    "#                       [0.6, 1.0, 0.2],\n",
    "#                       [0.0, 0.2, 1.5]])\n",
    "\n",
    "#     X_t   = sample_mv_t(mu, Sigma, n, nu=4.0)\n",
    "#     X_lap = sample_mv_laplace(mu, Sigma, n)\n",
    "#     X_nig = sample_mv_nig(mu, Sigma, n, alpha=1.8, delta=1.0)\n",
    "\n",
    "#     # 샘플 공분산 확인(유한표본 오차 있음)\n",
    "#     print(np.cov(X_t,   rowvar=False)[:2,:2])\n",
    "#     print(np.cov(X_lap, rowvar=False)[:2,:2])\n",
    "#     print(np.cov(X_nig, rowvar=False)[:2,:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517a3511",
   "metadata": {},
   "source": [
    "# make covariance matrix(different effective rank($k_{eff}$)) code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "6efc4049-0bbd-4751-a003-7bb0e708b16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eff_rank_by_diag_elements(x):\n",
    "    return np.sum(x)/np.max(x)\n",
    "\n",
    "dims = [2,5,10,20,50,100,500]\n",
    "\n",
    "covs = {}\n",
    "eranks = {}\n",
    "mus = {}\n",
    "\n",
    "for d in dims: \n",
    "    covs[d] = {}\n",
    "    eranks[d] = {}\n",
    "    mus[d] = np.array([0]*d+[1])\n",
    "    for u in ['full-rank','half-rank','low-rank']:\n",
    "        covs[d][u] = []\n",
    "        eranks[d][u] = []\n",
    "\n",
    "        if u=='full-rank':\n",
    "            eigs = np.repeat(0.1,d)\n",
    "        elif u=='half-rank':\n",
    "            if d%2==1:\n",
    "                eigs = np.array([d]*int(d/2)+[1]*int(d/2)+[0.5])\n",
    "            if d%2==0:\n",
    "                eigs = np.array([d]*int(d/2)+[0.01]*int(d/2))\n",
    "        elif u=='low-rank':\n",
    "            eigs = np.array([1]*max(1,int(d/10))+[0.0001]*(d-max(1,int(d/10))))\n",
    "        \n",
    "        eigs = eigs/np.sum(eigs)*0.10\n",
    "        trace_cov = np.sum(eigs)\n",
    "        eff_rank = eff_rank_by_diag_elements(eigs)\n",
    "        \n",
    "        covs[d][u].append(np.diag(eigs))\n",
    "        eranks[d][u].append(eff_rank)\n",
    "\n",
    "with open('means.pkl','wb') as f_:\n",
    "    pickle.dump(mus,f_)\n",
    "with open('covs.pkl','wb') as f_:\n",
    "    pickle.dump(covs,f_)\n",
    "with open('eranks.pkl','wb') as f_:\n",
    "    pickle.dump(eranks,f_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4085b7-9629-4844-b56f-2d5a375ec1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('means.pkl','rb') as f_:\n",
    "    mus = pickle.load(f_)\n",
    "with open('covs.pkl','rb') as f_:\n",
    "    covs = pickle.load(f_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e9b542-d129-4855-a301-1a99b1415083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distances distribution(avg:0.28/max:0.80/min:0.00)\n",
      "distances distribution(avg:0.26/max:1.06/min:0.00)\n",
      "distances distribution(avg:0.25/max:1.09/min:0.00)\n",
      "dimension 2, low-rank, gaussian done: 1576.4326882362366 seconds elapsed...\n",
      "distances distribution(avg:0.24/max:1.58/min:0.00)\n",
      "distances distribution(avg:0.22/max:2.97/min:0.00)\n",
      "distances distribution(avg:0.22/max:2.84/min:0.00)\n",
      "dimension 2, low-rank, t done: 3285.104064464569 seconds elapsed...\n",
      "distances distribution(avg:0.25/max:1.13/min:0.00)\n",
      "distances distribution(avg:0.24/max:2.27/min:0.00)\n",
      "distances distribution(avg:0.22/max:1.64/min:0.00)\n",
      "dimension 2, low-rank, laplace done: 4960.232086420059 seconds elapsed...\n",
      "distances distribution(avg:0.30/max:0.78/min:0.09)\n",
      "distances distribution(avg:0.30/max:0.73/min:0.06)\n",
      "distances distribution(avg:0.24/max:1.15/min:0.00)\n",
      "dimension 5, low-rank, gaussian done: 6411.91080904007 seconds elapsed...\n",
      "distances distribution(avg:0.26/max:1.82/min:0.06)\n",
      "distances distribution(avg:0.26/max:1.39/min:0.03)\n",
      "distances distribution(avg:0.22/max:1.74/min:0.00)\n",
      "dimension 5, low-rank, t done: 7914.651833534241 seconds elapsed...\n",
      "distances distribution(avg:0.26/max:1.24/min:0.00)\n",
      "distances distribution(avg:0.27/max:1.63/min:0.01)\n",
      "distances distribution(avg:0.23/max:1.91/min:0.00)\n",
      "dimension 5, low-rank, laplace done: 9350.890785694122 seconds elapsed...\n",
      "distances distribution(avg:0.31/max:0.52/min:0.12)\n",
      "distances distribution(avg:0.30/max:0.66/min:0.08)\n",
      "distances distribution(avg:0.25/max:0.96/min:0.01)\n",
      "dimension 10, low-rank, gaussian done: 11389.708764314651 seconds elapsed...\n",
      "distances distribution(avg:0.28/max:1.64/min:0.07)\n",
      "distances distribution(avg:0.26/max:1.81/min:0.04)\n",
      "distances distribution(avg:0.22/max:1.69/min:0.00)\n",
      "dimension 10, low-rank, t done: 13309.731200933456 seconds elapsed...\n",
      "distances distribution(avg:0.28/max:0.99/min:0.01)\n",
      "distances distribution(avg:0.26/max:1.02/min:0.01)\n",
      "distances distribution(avg:0.21/max:1.41/min:0.00)\n",
      "dimension 10, low-rank, laplace done: 14964.206584692001 seconds elapsed...\n",
      "distances distribution(avg:0.28/max:0.90/min:0.01)\n",
      "distances distribution(avg:0.26/max:1.24/min:0.01)\n",
      "distances distribution(avg:0.24/max:1.02/min:0.00)\n",
      "dimension 2, low-rank, gaussian done: 16735.810299396515 seconds elapsed...\n",
      "distances distribution(avg:0.25/max:1.86/min:0.01)\n",
      "distances distribution(avg:0.22/max:1.89/min:0.00)\n",
      "distances distribution(avg:0.22/max:1.60/min:0.00)\n",
      "dimension 2, low-rank, t done: 18579.674958467484 seconds elapsed...\n",
      "distances distribution(avg:0.25/max:1.55/min:0.00)\n",
      "distances distribution(avg:0.22/max:1.53/min:0.00)\n",
      "distances distribution(avg:0.22/max:1.25/min:0.00)\n",
      "dimension 2, low-rank, laplace done: 20374.951437950134 seconds elapsed...\n",
      "distances distribution(avg:0.30/max:0.59/min:0.07)\n",
      "distances distribution(avg:0.30/max:0.75/min:0.06)\n",
      "distances distribution(avg:0.25/max:1.17/min:0.00)\n",
      "dimension 5, low-rank, gaussian done: 21848.140210866928 seconds elapsed...\n",
      "distances distribution(avg:0.27/max:2.96/min:0.01)\n",
      "distances distribution(avg:0.26/max:1.48/min:0.05)\n",
      "distances distribution(avg:0.22/max:2.31/min:0.00)\n",
      "dimension 5, low-rank, t done: 23412.355675935745 seconds elapsed...\n",
      "distances distribution(avg:0.27/max:1.11/min:0.01)\n",
      "distances distribution(avg:0.26/max:1.33/min:0.00)\n",
      "distances distribution(avg:0.22/max:1.74/min:0.00)\n",
      "dimension 5, low-rank, laplace done: 24898.51797747612 seconds elapsed...\n",
      "distances distribution(avg:0.31/max:0.54/min:0.13)\n",
      "distances distribution(avg:0.30/max:0.62/min:0.06)\n",
      "distances distribution(avg:0.24/max:0.93/min:0.01)\n",
      "dimension 10, low-rank, gaussian done: 27162.25285577774 seconds elapsed...\n",
      "distances distribution(avg:0.27/max:1.73/min:0.06)\n",
      "distances distribution(avg:0.27/max:1.86/min:0.05)\n",
      "distances distribution(avg:0.22/max:2.87/min:0.00)\n",
      "dimension 10, low-rank, t done: 29204.171181440353 seconds elapsed...\n",
      "distances distribution(avg:0.28/max:0.96/min:0.01)\n",
      "distances distribution(avg:0.26/max:1.52/min:0.00)\n",
      "distances distribution(avg:0.22/max:1.94/min:0.00)\n",
      "dimension 10, low-rank, laplace done: 30934.268841981888 seconds elapsed...\n",
      "distances distribution(avg:0.28/max:0.79/min:0.02)\n",
      "distances distribution(avg:0.25/max:0.96/min:0.00)\n",
      "distances distribution(avg:0.25/max:1.27/min:0.00)\n",
      "dimension 2, low-rank, gaussian done: 32641.516221046448 seconds elapsed...\n",
      "distances distribution(avg:0.24/max:2.32/min:0.01)\n",
      "distances distribution(avg:0.22/max:1.99/min:0.00)\n",
      "distances distribution(avg:0.22/max:1.59/min:0.00)\n",
      "dimension 2, low-rank, t done: 34437.1700758934 seconds elapsed...\n",
      "distances distribution(avg:0.24/max:1.51/min:0.00)\n",
      "distances distribution(avg:0.22/max:1.34/min:0.00)\n",
      "distances distribution(avg:0.22/max:1.51/min:0.00)\n",
      "dimension 2, low-rank, laplace done: 36155.00593543053 seconds elapsed...\n",
      "distances distribution(avg:0.30/max:0.71/min:0.08)\n",
      "distances distribution(avg:0.30/max:0.78/min:0.05)\n",
      "distances distribution(avg:0.26/max:1.18/min:0.00)\n",
      "dimension 5, low-rank, gaussian done: 37670.1315305233 seconds elapsed...\n",
      "distances distribution(avg:0.28/max:2.14/min:0.04)\n",
      "distances distribution(avg:0.26/max:1.95/min:0.03)\n",
      "distances distribution(avg:0.22/max:2.77/min:0.00)\n",
      "dimension 5, low-rank, t done: 39246.354143857956 seconds elapsed...\n",
      "distances distribution(avg:0.27/max:0.89/min:0.01)\n",
      "distances distribution(avg:0.26/max:1.39/min:0.00)\n",
      "distances distribution(avg:0.21/max:1.25/min:0.00)\n",
      "dimension 5, low-rank, laplace done: 40653.94342780113 seconds elapsed...\n",
      "distances distribution(avg:0.31/max:0.62/min:0.13)\n",
      "distances distribution(avg:0.30/max:0.62/min:0.04)\n",
      "distances distribution(avg:0.24/max:1.31/min:0.01)\n",
      "dimension 10, low-rank, gaussian done: 42867.26298570633 seconds elapsed...\n",
      "distances distribution(avg:0.28/max:2.22/min:0.06)\n",
      "distances distribution(avg:0.28/max:2.66/min:0.03)\n",
      "distances distribution(avg:0.23/max:3.05/min:0.00)\n",
      "dimension 10, low-rank, t done: 44759.879382133484 seconds elapsed...\n",
      "distances distribution(avg:0.28/max:0.93/min:0.01)\n",
      "distances distribution(avg:0.27/max:1.23/min:0.00)\n",
      "distances distribution(avg:0.23/max:1.70/min:0.00)\n",
      "dimension 10, low-rank, laplace done: 46574.64943981171 seconds elapsed...\n",
      "distances distribution(avg:0.28/max:1.14/min:0.01)\n",
      "distances distribution(avg:0.25/max:0.97/min:0.00)\n",
      "distances distribution(avg:0.25/max:1.19/min:0.00)\n",
      "dimension 2, low-rank, gaussian done: 48240.6120903492 seconds elapsed...\n",
      "distances distribution(avg:0.25/max:1.41/min:0.01)\n",
      "distances distribution(avg:0.23/max:2.63/min:0.00)\n",
      "distances distribution(avg:0.23/max:1.92/min:0.00)\n",
      "dimension 2, low-rank, t done: 50059.7368414402 seconds elapsed...\n",
      "distances distribution(avg:0.26/max:1.75/min:0.01)\n",
      "distances distribution(avg:0.22/max:1.62/min:0.00)\n",
      "distances distribution(avg:0.22/max:1.57/min:0.00)\n",
      "dimension 2, low-rank, laplace done: 51824.63516998291 seconds elapsed...\n",
      "distances distribution(avg:0.30/max:0.66/min:0.07)\n",
      "distances distribution(avg:0.30/max:0.80/min:0.05)\n",
      "distances distribution(avg:0.25/max:0.97/min:0.00)\n",
      "dimension 5, low-rank, gaussian done: 53363.157267570496 seconds elapsed...\n",
      "distances distribution(avg:0.27/max:2.13/min:0.02)\n",
      "distances distribution(avg:0.26/max:1.92/min:0.04)\n",
      "distances distribution(avg:0.23/max:2.88/min:0.00)\n",
      "dimension 5, low-rank, t done: 54883.23692393303 seconds elapsed...\n",
      "distances distribution(avg:0.26/max:1.01/min:0.00)\n",
      "distances distribution(avg:0.26/max:1.07/min:0.01)\n",
      "distances distribution(avg:0.21/max:1.42/min:0.00)\n",
      "dimension 5, low-rank, laplace done: 56306.78903770447 seconds elapsed...\n",
      "distances distribution(avg:0.31/max:0.55/min:0.13)\n",
      "distances distribution(avg:0.30/max:0.68/min:0.07)\n",
      "distances distribution(avg:0.25/max:1.01/min:0.00)\n",
      "dimension 10, low-rank, gaussian done: 58425.8799328804 seconds elapsed...\n",
      "distances distribution(avg:0.27/max:1.52/min:0.06)\n",
      "distances distribution(avg:0.27/max:2.93/min:0.04)\n",
      "distances distribution(avg:0.22/max:1.64/min:0.00)\n",
      "dimension 10, low-rank, t done: 60438.37051463127 seconds elapsed...\n",
      "distances distribution(avg:0.28/max:0.79/min:0.00)\n",
      "distances distribution(avg:0.27/max:1.16/min:0.00)\n",
      "distances distribution(avg:0.23/max:1.96/min:0.00)\n",
      "dimension 10, low-rank, laplace done: 62252.31665229797 seconds elapsed...\n",
      "distances distribution(avg:0.27/max:0.74/min:0.01)\n",
      "distances distribution(avg:0.24/max:0.96/min:0.00)\n",
      "distances distribution(avg:0.26/max:1.02/min:0.00)\n",
      "dimension 2, low-rank, gaussian done: 63963.43118548393 seconds elapsed...\n",
      "distances distribution(avg:0.25/max:2.12/min:0.00)\n",
      "distances distribution(avg:0.23/max:2.30/min:0.00)\n",
      "distances distribution(avg:0.23/max:2.06/min:0.00)\n",
      "dimension 2, low-rank, t done: 65815.9703013897 seconds elapsed...\n",
      "distances distribution(avg:0.25/max:1.25/min:0.00)\n",
      "distances distribution(avg:0.23/max:1.53/min:0.00)\n",
      "distances distribution(avg:0.23/max:2.03/min:0.00)\n",
      "dimension 2, low-rank, laplace done: 67529.41769123077 seconds elapsed...\n",
      "distances distribution(avg:0.30/max:0.61/min:0.04)\n",
      "distances distribution(avg:0.30/max:0.77/min:0.06)\n",
      "distances distribution(avg:0.25/max:1.16/min:0.00)\n",
      "dimension 5, low-rank, gaussian done: 69187.82447218895 seconds elapsed...\n",
      "distances distribution(avg:0.26/max:1.66/min:0.03)\n",
      "distances distribution(avg:0.26/max:1.41/min:0.03)\n",
      "distances distribution(avg:0.23/max:1.60/min:0.00)\n",
      "dimension 5, low-rank, t done: 70938.33420872688 seconds elapsed...\n",
      "distances distribution(avg:0.27/max:1.34/min:0.01)\n",
      "distances distribution(avg:0.26/max:1.19/min:0.00)\n",
      "distances distribution(avg:0.23/max:1.53/min:0.00)\n",
      "dimension 5, low-rank, laplace done: 72451.33855676651 seconds elapsed...\n",
      "distances distribution(avg:0.31/max:0.53/min:0.12)\n",
      "distances distribution(avg:0.30/max:0.70/min:0.06)\n",
      "distances distribution(avg:0.25/max:0.99/min:0.01)\n",
      "dimension 10, low-rank, gaussian done: 74539.6207883358 seconds elapsed...\n",
      "distances distribution(avg:0.28/max:2.33/min:0.06)\n",
      "distances distribution(avg:0.27/max:2.27/min:0.04)\n",
      "distances distribution(avg:0.23/max:2.96/min:0.00)\n",
      "dimension 10, low-rank, t done: 76442.47038531303 seconds elapsed...\n",
      "distances distribution(avg:0.27/max:1.10/min:0.01)\n",
      "distances distribution(avg:0.26/max:1.27/min:0.01)\n",
      "distances distribution(avg:0.23/max:1.66/min:0.00)\n",
      "dimension 10, low-rank, laplace done: 78193.08567357063 seconds elapsed...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n_sampless = [1000] #,300,25,100]\n",
    "sampling_process_iter = 100\n",
    "\n",
    "pop_dist_types = ['gaussian','t','laplace']\n",
    "\n",
    "tic = time.time()\n",
    "\n",
    "n_rand = 10\n",
    "\n",
    "dims = [2,5,10,20,50,100,500] \n",
    "for i_c in range(n_rand):\n",
    "    for n_samples in n_sampless:\n",
    "        result_compr = {}\n",
    "        for dim in dims:\n",
    "            manifold = Hypersphere(dim)\n",
    "            seed = 50+dim+i_c\n",
    "            np.random.seed(seed)\n",
    "            result_compr[dim] = {}\n",
    "            \n",
    "            mu = mus[dim]\n",
    "                    \n",
    "            for pop_dist_type in pop_dist_types:\n",
    "                result_compr[dim][pop_dist_type] = {}\n",
    "                \n",
    "                for u,u_covs in covs[dim].items():\n",
    "                    \n",
    "                    res_ = riemannian_sample_variance_comparison(\n",
    "                        manifold=manifold,\n",
    "                        mu=mu,\n",
    "                        cov=u_covs[0],\n",
    "                        pop_dist_type=pop_dist_type,\n",
    "                        n_samples=n_samples,\n",
    "                        sample_iter=sampling_process_iter\n",
    "                    )\n",
    "                    result_compr[dim][pop_dist_type][u] = res_\n",
    "                print(f'dimension {dim}, {u}, {pop_dist_type} done: {time.time()-tic} seconds elapsed...')\n",
    "\n",
    "        with open(f'sphere/varofsamplemean_simul_n{n_samples}_cov3type_mest_{i_c}.pkl','wb') as f_:\n",
    "            pickle.dump(result_compr,f_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97092fc9",
   "metadata": {},
   "source": [
    "## different random seed test (d=10, low-rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13398b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rand = 10\n",
    "n_sampless = [1000] #,300,25,100]\n",
    "sampling_process_iter = 100\n",
    "\n",
    "pop_dist_types = ['gaussian','t','laplace']\n",
    "\n",
    "tic = time.time()\n",
    "\n",
    "dims = [10]\n",
    "for i_c in range(n_rand):\n",
    "    for n_samples in n_sampless:\n",
    "        result_compr = {}\n",
    "        for dim in dims:\n",
    "            seed = 5300+dim+i_c\n",
    "            np.random.seed(seed)\n",
    "            manifold = Hypersphere(dim)\n",
    "            result_compr[dim] = {}\n",
    "            \n",
    "            mu = np.array([0.0]*dim)\n",
    "                    \n",
    "            for pop_dist_type in pop_dist_types:\n",
    "                result_compr[dim][pop_dist_type] = {}\n",
    "                \n",
    "                for u,u_covs in covs[dim].items():\n",
    "                    try:\n",
    "                        res_ = riemannian_sample_variance_comparison(\n",
    "                            manifold=manifold,\n",
    "                            mu=mu,\n",
    "                            cov=u_covs[0],\n",
    "                            pop_dist_type=pop_dist_type,\n",
    "                            n_samples=n_samples,\n",
    "                            sample_iter=sampling_process_iter\n",
    "                        )\n",
    "                        result_compr[dim][pop_dist_type][u] = res_\n",
    "                        print(f'[{i_c}] dimension {dim}, {u}, {pop_dist_type} done: {time.time()-tic} seconds elapsed...')\n",
    "                    except ValueError:\n",
    "                        print(f'[{i_c}] [dimension {dim}, {u}, {pop_dist_type}] norm exploding error ...')\n",
    "                        \n",
    "                \n",
    "        with open(f'old_rawdata/sphere_3_seedtest/varofsamplemean_simul_n{n_samples}_dim10_cov3type_mest_{i_c}.pkl','wb') as f_:\n",
    "            pickle.dump(result_compr,f_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0f5012",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rand = 10\n",
    "n_sampless = [1000] #,300,25,100]\n",
    "sampling_process_iter = 100\n",
    "\n",
    "pop_dist_types = ['gaussian','t','laplace']\n",
    "\n",
    "tic = time.time()\n",
    "\n",
    "dims = [10]\n",
    "for i_c in range(n_rand):\n",
    "    for n_samples in n_sampless:\n",
    "        result_compr = {}\n",
    "        for dim in dims:\n",
    "            \n",
    "            manifold = Hypersphere(dim)\n",
    "            result_compr[dim] = {}\n",
    "            \n",
    "            mu = np.array([0.0]*dim)\n",
    "                    \n",
    "            for pop_dist_type in pop_dist_types:\n",
    "                result_compr[dim][pop_dist_type] = {}\n",
    "                \n",
    "                for u,u_covs in covs[dim].items():\n",
    "                    try:\n",
    "                        res_ = riemannian_sample_variance_comparison(\n",
    "                            manifold=manifold,\n",
    "                            mu=mu,\n",
    "                            cov=u_covs[0],\n",
    "                            pop_dist_type=pop_dist_type,\n",
    "                            n_samples=n_samples,\n",
    "                            sample_iter=sampling_process_iter\n",
    "                        )\n",
    "                        result_compr[dim][pop_dist_type][u] = res_\n",
    "                        print(f'[{i_c}] dimension {dim}, {u}, {pop_dist_type} done: {time.time()-tic} seconds elapsed...')\n",
    "                    except ValueError:\n",
    "                        print(f'[{i_c}] [dimension {dim}, {u}, {pop_dist_type}] norm exploding error ...')\n",
    "                        \n",
    "                \n",
    "        with open(f'old_rawdata/sphere_2_seedtest/varofsamplemean_simul_n{n_samples}_dim10_cov3type_mest_{i_c}.pkl','wb') as f_:\n",
    "            pickle.dump(result_compr,f_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
