{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "945d74e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(os.path.join(*str(os.getcwd()).split('\\\\')[:-1]).replace(':',':\\\\'))\n",
    "import riemannian_robust_m_estimator as rrm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import os, time, pickle\n",
    "import math\n",
    "from scipy.stats import chi2\n",
    "\n",
    "import geomstats.backend as gs\n",
    "import geomstats.visualization as visualization\n",
    "from geomstats.geometry.euclidean import Euclidean\n",
    "from geomstats.geometry.hyperboloid import Hyperboloid\n",
    "from geomstats.geometry.hypersphere import Hypersphere\n",
    "from geomstats.geometry.poincare_ball import PoincareBall\n",
    "from geomstats.geometry.poincare_half_space import PoincareHalfSpace\n",
    "from geomstats.geometry.spd_matrices import SPDMatrices\n",
    "from geomstats.geometry.special_euclidean import SpecialEuclidean\n",
    "from geomstats.geometry.special_orthogonal import SpecialOrthogonal\n",
    "from geomstats.learning.frechet_mean import FrechetMean\n",
    "from geomstats.learning.geometric_median import GeometricMedian\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "489df9c4-b4fa-495b-bde4-51246f1f3827",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "logging.getLogger('root').setLevel(logging.ERROR)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46fb64a4",
   "metadata": {},
   "source": [
    "# Sample mean simulation code\n",
    "> use hyperboloid manifold\n",
    "1) Set $\\mu$, and covariance matrix $\\Sigma$ which is the same as sphere/euclidean.\n",
    "2) Generate Lorentz gram schmidt matrix $\\mathbf{E}$ for $\\mu$(tangent point), $\\mu$=(1,0,0,...)\n",
    "3) Generate random samples $\\mathbf{U} \\in \\mathbb{R}^{n \\times d}$ from pre-specified distribution with $\\mu$, covariance matrix $\\Sigma$\n",
    "4) $\\mathbf{X} = \\mathbf{EU}^T$, and get $\\mathbf{X}^* = Exp_{\\mu}(\\mathbf{X})$ for manifold-valued sample, $Exp$ is typical exponential map of the hyperboloid manifold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d38d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------- 유틸 ---------\n",
    "def chol_psd_for_cov(Sigma, scale=1.0, eps=1e-12):\n",
    "    \"\"\"L L^T = Sigma / scale 가 되도록 수치안정 고유분해 기반 'Cholesky-like'.\"\"\"\n",
    "    Sigma = np.asarray(Sigma, float)\n",
    "    vals, vecs = np.linalg.eigh(Sigma)\n",
    "    vals = np.clip(vals, eps, None)\n",
    "    target = vals / float(scale)\n",
    "    return vecs @ np.diag(np.sqrt(target)) @ vecs.T\n",
    "\n",
    "# Inverse-Gaussian(μ, λ) 샘플러 (Michael–Schucany–Haas)\n",
    "def sample_inverse_gaussian(mu, lam, size, rng):\n",
    "    v = rng.standard_normal(size=size)\n",
    "    y = v**2\n",
    "    x = mu + (mu**2 * y)/(2*lam) - (mu/(2*lam))*np.sqrt(4*mu*lam*y + mu**2 * y**2)\n",
    "    u = rng.uniform(size=size)\n",
    "    w = np.where(u <= mu/(mu + x), x, (mu**2)/x)\n",
    "    return w\n",
    "\n",
    "# --------- 1) Student-t_ν (ν>2) ---------\n",
    "def sample_mv_t(mean, Sigma, n, nu, rng=None):\n",
    "    \"\"\"\n",
    "    mean: (d,), Sigma: (d,d), nu>2\n",
    "    공분산이 정확히 Sigma가 되도록 자동 스케일.\n",
    "    \"\"\"\n",
    "    assert nu > 2, \"Student-t에서 공분산 유한/매칭은 ν>2 필요.\"\n",
    "    rng = np.random.default_rng() if rng is None else rng\n",
    "    E_W = nu / (nu - 2.0)\n",
    "    L = chol_psd_for_cov(Sigma, scale=E_W)\n",
    "    G = rng.chisquare(df=nu, size=n)          # χ²_ν\n",
    "    W = nu / G                                # mixture\n",
    "    Z = rng.standard_normal((n, mean.size))   # N(0, I)\n",
    "    X = mean + (np.sqrt(W)[:, None] * (Z @ L.T))\n",
    "    return X\n",
    "\n",
    "# --------- 2) Elliptical Laplace ---------\n",
    "def sample_mv_laplace(mean, Sigma, n, rng=None):\n",
    "    \"\"\"\n",
    "    타원형 Laplace: X|W ~ N(mean, W Sigma), W ~ Exp(1), E[W]=1\n",
    "    -> L L^T = Sigma\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng() if rng is None else rng\n",
    "    L = chol_psd_for_cov(Sigma, scale=1.0)\n",
    "    W = rng.exponential(scale=1.0, size=n)\n",
    "    Z = rng.standard_normal((n, mean.size))\n",
    "    X = mean + (np.sqrt(W)[:, None] * (Z @ L.T))\n",
    "    return X\n",
    "\n",
    "# --------- 3) NIG (대칭형, β=0) ---------\n",
    "def sample_mv_nig(mean, Sigma, n, alpha=1.5, delta=1.0, rng=None):\n",
    "    \"\"\"\n",
    "    대칭 NIG: W ~ IG(μ=δ/α, λ=δα), E[W]=δ/α\n",
    "    -> L L^T = Sigma / (δ/α)\n",
    "    (왜도 β!=0 도 가능하지만, 그 경우 Cov에 Var(W)ββ^T가 추가되므로 별도 스케일 조정 필요)\n",
    "    \"\"\"\n",
    "    assert alpha > 0 and delta > 0\n",
    "    rng = np.random.default_rng() if rng is None else rng\n",
    "    muW = delta / alpha\n",
    "    lamW = delta * alpha\n",
    "    L = chol_psd_for_cov(Sigma, scale=muW)\n",
    "    W = sample_inverse_gaussian(mu=muW, lam=lamW, size=n, rng=rng)\n",
    "    Z = rng.standard_normal((n, mean.size))\n",
    "    X = mean + (np.sqrt(W)[:, None] * (Z @ L.T))\n",
    "    return X\n",
    "\n",
    "# # --------- 사용 예시 ---------\n",
    "# if __name__ == \"__main__\":\n",
    "#     d, n = 3, 10000\n",
    "#     mu = np.zeros(d)\n",
    "#     Sigma = np.array([[1.0, 0.6, 0.0],\n",
    "#                       [0.6, 1.0, 0.2],\n",
    "#                       [0.0, 0.2, 1.5]])\n",
    "\n",
    "#     X_t   = sample_mv_t(mu, Sigma, n, nu=4.0)\n",
    "#     X_lap = sample_mv_laplace(mu, Sigma, n)\n",
    "#     X_nig = sample_mv_nig(mu, Sigma, n, alpha=1.8, delta=1.0)\n",
    "\n",
    "#     # 샘플 공분산 확인(유한표본 오차 있음)\n",
    "#     print(np.cov(X_t,   rowvar=False)[:2,:2])\n",
    "#     print(np.cov(X_lap, rowvar=False)[:2,:2])\n",
    "#     print(np.cov(X_nig, rowvar=False)[:2,:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd2a10c-eb29-424d-8c20-913c5d90f526",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def riemannian_sample_variance_comparison_hyperbolic(manifold, mu, cov, n_samples, pop_dist_type = 'gaussian', sample_iter=100, seed=None):\n",
    "\n",
    "    cutoff_ptiles = [5,10,15,20,25,30,35,40,45,50,60,70,75,80,82.13,85,90,95,99,99.73]\n",
    "    max_iter_graddescent = 2048\n",
    "    epsilon = 1e-7\n",
    "    results = {\n",
    "        'mean':{'fm':[],'gm':[],\n",
    "               },\n",
    "        'var':{'fm':[],'gm':[],\n",
    "               },\n",
    "        'sq_bias':{'fm':[],'gm':[],\n",
    "               },\n",
    "        'sample_mean_var':{},\n",
    "        'sample_var_mean':{},\n",
    "        'sample_var_var':{},\n",
    "        'sq_bias_mean':{},\n",
    "        'sq_bias_var':{},\n",
    "        'invalid':[],\n",
    "        'cov':cov,\n",
    "        'distances':[],\n",
    "        'cutoff':{}\n",
    "    }\n",
    "    for cutoff_pct in cutoff_ptiles:\n",
    "        results['cutoff'][cutoff_pct] = []\n",
    "    m_est_type_lb = {'mono':('huber',1.345),'softr':('cauchy',2.3849),'hardr':('biweight',4.6851)}\n",
    "    for i in range(sample_iter):\n",
    "        data = sample_hyperbolic_gaussian_t_laplace(manifold=manifold, mu=mu, Sigma=cov, sample_dist=pop_dist_type, n_samples=n_samples, seed=seed)\n",
    "                \n",
    "        dists = manifold.metric.dist(data, mu)\n",
    "        results['distances'].append(dists)\n",
    "        if i==0:\n",
    "            print(f'distances distribution(avg:{np.mean(dists):.2f}/max:{np.max(dists):.2f}/min:{np.min(dists):.2f})')\n",
    "        fm = rrm.RiemannianRobustMestimator(\n",
    "                    space=manifold,\n",
    "                    method='default',\n",
    "                    m_estimator='custom',\n",
    "                    critical_value=None,\n",
    "                    init_point_method='mean-projection',\n",
    "                )\n",
    "        fm.set_loss(squaredist)\n",
    "        fm.set(init_step_size=0.5, max_iter=max_iter_graddescent, epsilon=epsilon)\n",
    "        fm.fit(data)\n",
    "                \n",
    "        # fm = FrechetMean(manifold)\n",
    "        # fm.set(init_step_size=0.5, max_iter=max_iter_graddescent, epsilon=epsilon)\n",
    "        # fm.fit(data)\n",
    "        if len(fm.estimate_.losses)>max_iter_graddescent-50:\n",
    "            results['invalid'].append(f'{res_lb}_{i}')\n",
    "            results['mean'][res_lb].append(gs.array([9999]*len(data[0])))\n",
    "            results['var'][res_lb].append(9999)\n",
    "            results['sq_bias'][res_lb].append(9999)\n",
    "        else:\n",
    "            results['mean']['fm'].append(fm.estimate_.x)\n",
    "            results['var']['fm'].append(\n",
    "                rrm.riemannian_variance(manifold, data, base=fm.estimate_.x)\n",
    "            )\n",
    "            bias = fm.estimate_.x - mu\n",
    "            results['sq_bias']['fm'].append(\n",
    "                bias.T @ bias\n",
    "            )\n",
    "    \n",
    "        gm = GeometricMedian(manifold, lr=0.5, max_iter=max_iter_graddescent, epsilon=epsilon)\n",
    "        gm.fit(data)\n",
    "        results['mean']['gm'].append(gm.estimate_)\n",
    "        results['var']['gm'].append(\n",
    "            rrm.riemannian_variance(manifold, data, base=gm.estimate_)\n",
    "        )\n",
    "        bias = gm.estimate_ - mu\n",
    "        results['sq_bias']['gm'].append(\n",
    "            bias.T @ bias\n",
    "        )\n",
    "        \n",
    "        for cutoff_pct in cutoff_ptiles:\n",
    "            ptile_ix = int(len(dists)*cutoff_pct/100)-1\n",
    "            c = np.sort(dists)[ptile_ix]\n",
    "            results['cutoff'][cutoff_pct].append(c)\n",
    "            for m_est_tp,(m_est_lb,c_95) in m_est_type_lb.items(): \n",
    "                res_lb = f'{m_est_tp}_{cutoff_pct:.2f}'\n",
    "                if res_lb not in results['mean'].keys():\n",
    "                    results['mean'][res_lb] = []\n",
    "                    results['var'][res_lb] = []\n",
    "                    results['sq_bias'][res_lb] = []\n",
    "                    \n",
    "                m_est = rrm.RiemannianRobustMestimator(\n",
    "                    space=manifold, method='default', m_estimator=m_est_lb, critical_value=c, init_point_method='mean-projection'\n",
    "                )\n",
    "                m_est.set(init_step_size=5 if m_est_lb == 'biweight' else 0.5, max_iter=max_iter_graddescent, epsilon=epsilon)\n",
    "                m_est.fit(data)\n",
    "                if len(m_est.estimate_.losses)>max_iter_graddescent-50:\n",
    "                    results['invalid'].append(f'{res_lb}_{i}')\n",
    "                    results['mean'][res_lb].append(gs.array([9999]*len(data[0])))\n",
    "                    results['var'][res_lb].append(9999)\n",
    "                    results['sq_bias'][res_lb].append(9999)\n",
    "                else:\n",
    "                    results['mean'][res_lb].append(m_est.estimate_.x)\n",
    "                    results['var'][res_lb].append(\n",
    "                        rrm.riemannian_variance(manifold, data, base=m_est.estimate_.x)\n",
    "                    )\n",
    "                    bias = m_est.estimate_.x - mu\n",
    "                    results['sq_bias'][res_lb].append(\n",
    "                        bias.T @ bias\n",
    "                    )\n",
    "                \n",
    "\n",
    "    for lb in results['mean'].keys():\n",
    "        results['sample_mean_var'][lb] = rrm.riemannian_variance(manifold, gs.array([i for i in results['mean'][lb] if i[0]!=9999])),\n",
    "        results['sample_var_mean'][lb] = np.mean([i for i in results['var'][lb] if i!=9999]),\n",
    "        results['sample_var_var'][lb] = np.var([i for i in results['var'][lb] if i!=9999]),\n",
    "        results['sq_bias_mean'][lb] = np.mean([i for i in results['sq_bias'][lb] if i!=9999]),\n",
    "        results['sq_bias_var'][lb] = np.var([i for i in results['sq_bias'][lb] if i!=9999]),\n",
    "        \n",
    "    return results\n",
    "\n",
    "def squaredist(space,points,base,critical_value,weights=None,loss_and_grad=True):\n",
    "    n = len(points)\n",
    "    logs = space.metric.log(points,base)\n",
    "    dists = space.metric.dist(points,base)\n",
    "\n",
    "    loss = gs.sum(dists**2)/n\n",
    "    if not loss_and_grad:\n",
    "        return loss\n",
    "    grad = -2*gs.sum(logs,axis=0)/n\n",
    "    return loss, space.to_tangent(grad,base)\n",
    "    \n",
    "\n",
    "# -----------------------------\n",
    "# Hyperbolic Gaussian sampler\n",
    "# -----------------------------\n",
    "def lorentz_gram_schmidt(E0, manifold, base_point, d=None):\n",
    "    \"\"\"\n",
    "    Lorentzian Gram–Schmidt on columns of E0 (ambient vectors),\n",
    "    projected to T_{base_point}. Build d tangent orthonormal columns.\n",
    "\n",
    "    E0: (d_plus_1, n_candidates) with n_candidates >= d (e.g., 4*d)\n",
    "    d:  manifold dimension (if None -> infer from ambient)\n",
    "    \"\"\"\n",
    "    M = E0.shape[0]                    # ambient dim = d+1\n",
    "    if d is None:\n",
    "        d = M - 1                      # hyperbolic dim\n",
    "\n",
    "    E = gs.zeros((M, d))               # target frame (d columns)\n",
    "    k = 0                              # how many columns we have built\n",
    "\n",
    "    for j in range(E0.shape[1]):       # iterate over candidates (e.g., 4d)\n",
    "        # project to tangent at base_point\n",
    "        v = manifold.to_tangent(E0[:, j], base_point=base_point)\n",
    "\n",
    "        # Lorentzian Gram–Schmidt\n",
    "        for i in range(k):\n",
    "            ip = manifold.metric.inner_product(v, E[:, i], base_point=base_point)\n",
    "            v = v - ip * E[:, i]\n",
    "\n",
    "        n2 = manifold.metric.inner_product(v, v, base_point=base_point)\n",
    "        if n2 > 1e-12:\n",
    "            E[:, k] = v / gs.sqrt(n2)  # <- numpy/torch backend: 슬라이스 대입\n",
    "            k += 1\n",
    "            if k == d:\n",
    "                break\n",
    "\n",
    "    if k < d:\n",
    "        raise RuntimeError(\n",
    "            f\"Failed to build a full Lorentz-orthonormal frame: got {k}/{d}.\"\n",
    "        )\n",
    "    return E\n",
    "\n",
    "def sample_hyperbolic_gaussian_t_laplace(manifold, mu, Sigma, sample_dist='gaussian', n_samples=1024, seed=None):\n",
    "    \"\"\"\n",
    "    mu:        (d+1,) point on hyperboloid (extrinsic coords), <mu,mu>_L = -1, mu[0] > 0\n",
    "    Sigma:     (d,d) SPD covariance in an orthonormal frame of T_mu (d = manifold dim)\n",
    "    n_samples: number of samples\n",
    "    returns:   (n_samples, d+1) points on the hyperboloid\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        gs.random.seed(seed)\n",
    "        my_rng = np.random.default_rng(seed=seed)\n",
    "    else:\n",
    "        my_rng = np.random.default_rng()\n",
    "    d_plus_1 = mu.shape[0]\n",
    "    d = d_plus_1 - 1\n",
    "\n",
    "    # 1) Build an orthonormal frame E of T_mu (columns e_1..e_d)\n",
    "    #    Start from random ambient directions\n",
    "    E0 = gs.random.normal(size=(d_plus_1, 4*d))  # oversample, then GS\n",
    "    E = lorentz_gram_schmidt(E0, manifold=manifold, base_point=mu, d=d)  # (d+1, d)\n",
    "\n",
    "    # 2) Tangent Gaussian: u ~ N(0, Sigma) in R^d, then v = E @ u in T_mu\n",
    "    # L = gs.linalg.cholesky(Sigma)                      # (d, d)\n",
    "    # z = gs.random.normal(size=(n_samples, d))          # (N, d)\n",
    "    # u = gs.matmul(z, gs.transpose(L))                  # (N, d)\n",
    "    if sample_dist=='gaussian':\n",
    "        u = np.random.multivariate_normal(mean=np.zeros(d),cov=Sigma,size=n_samples)\n",
    "    elif sample_dist=='t':\n",
    "        u = sample_mv_t(mean=np.zeros(d), Sigma=Sigma, n=n_samples, nu=4, rng=my_rng)\n",
    "    elif sample_dist=='laplace':\n",
    "        u = sample_mv_laplace(mean=np.zeros(d), Sigma=Sigma, n=n_samples, rng=my_rng)\n",
    "    v = gs.matmul(E, gs.transpose(u))                  # (d+1, N)\n",
    "    v = gs.transpose(v)                                # (N, d+1)\n",
    "\n",
    "    # 3) Exponential map to the manifold\n",
    "    x = manifold.metric.exp(v, base_point=mu)          # (N, d+1)\n",
    "    return x\n",
    "\n",
    "# -----------------------------\n",
    "# Example usage\n",
    "# -----------------------------\n",
    "# Set backend if needed (numpy/jax/pytorch). Default는 numpy 백엔드.\n",
    "# gs.set_backend('numpy')  # or 'pytorch', 'jax'\n",
    "\n",
    "d = 4\n",
    "manifold = Hyperboloid(dim=d,)\n",
    "# metric = manifold.metric\n",
    "\n",
    "# # Choose a convenient \"origin\" point on the hyperboloid: (1, 0, ..., 0)\n",
    "mu = gs.array([1.0]+[0.0]*d)\n",
    "\n",
    "# Tangent-space covariance (d x d), in an orthonormal frame at mu\n",
    "Sigma = gs.array([[0.20, 0.05],\n",
    "                  [0.05, 0.10]])\n",
    "\n",
    "# X = sample_hyperbolic_gaussian(mu, Sigma, n_samples=5000, seed=42)\n",
    "\n",
    "spdm = SPDMatrices(d)\n",
    "Sigma = spdm.random_point(1)\n",
    "x1 = sample_hyperbolic_gaussian_t_laplace(manifold, mu, Sigma, sample_dist='gaussian', n_samples=1024, seed=None)\n",
    "manifold.belongs(x1).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d2f06106-02a3-4bdb-8371-1b04f1e972e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('means.pkl','rb') as f_:\n",
    "    mus = pickle.load(f_)\n",
    "with open('covs.pkl','rb') as f_:\n",
    "    covs = pickle.load(f_)\n",
    "with open('eranks.pkl','rb') as f_:\n",
    "    eranks = pickle.load(f_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e9b542-d129-4855-a301-1a99b1415083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distances distribution(avg:0.31/max:0.51/min:0.17)\n",
      "dimension 20, full-rank, gaussian done: 1071.0331754684448 seconds elapsed...\n",
      "distances distribution(avg:0.31/max:0.55/min:0.13)\n",
      "dimension 20, half-rank, gaussian done: 1893.4208524227142 seconds elapsed...\n",
      "distances distribution(avg:0.28/max:0.87/min:0.01)\n",
      "dimension 20, low-rank, gaussian done: 2512.824872493744 seconds elapsed...\n",
      "distances distribution(avg:0.27/max:1.50/min:0.07)\n",
      "dimension 20, full-rank, t done: 2969.351891517639 seconds elapsed...\n",
      "distances distribution(avg:0.27/max:2.58/min:0.06)\n",
      "dimension 20, half-rank, t done: 3481.461434364319 seconds elapsed...\n",
      "distances distribution(avg:0.24/max:1.88/min:0.01)\n",
      "dimension 20, low-rank, t done: 4060.6586339473724 seconds elapsed...\n",
      "distances distribution(avg:0.28/max:0.84/min:0.01)\n",
      "dimension 20, full-rank, laplace done: 4343.976060152054 seconds elapsed...\n",
      "distances distribution(avg:0.26/max:0.91/min:0.01)\n",
      "dimension 20, half-rank, laplace done: 4609.567365646362 seconds elapsed...\n",
      "distances distribution(avg:0.25/max:1.92/min:0.00)\n",
      "dimension 20, low-rank, laplace done: 5046.171285152435 seconds elapsed...\n",
      "distances distribution(avg:0.31/max:0.42/min:0.22)\n",
      "dimension 50, full-rank, gaussian done: 7644.661323070526 seconds elapsed...\n",
      "distances distribution(avg:0.32/max:0.48/min:0.20)\n",
      "dimension 50, half-rank, gaussian done: 9729.411619186401 seconds elapsed...\n",
      "distances distribution(avg:0.29/max:0.58/min:0.07)\n",
      "dimension 50, low-rank, gaussian done: 10833.989478588104 seconds elapsed...\n",
      "distances distribution(avg:0.28/max:1.10/min:0.09)\n",
      "dimension 50, full-rank, t done: 11616.286120176315 seconds elapsed...\n",
      "distances distribution(avg:0.28/max:1.74/min:0.09)\n",
      "dimension 50, half-rank, t done: 12440.871875047684 seconds elapsed...\n",
      "distances distribution(avg:0.27/max:1.87/min:0.04)\n",
      "dimension 50, low-rank, t done: 13410.615477085114 seconds elapsed...\n",
      "distances distribution(avg:0.27/max:0.87/min:0.01)\n",
      "dimension 50, full-rank, laplace done: 14034.149983406067 seconds elapsed...\n",
      "distances distribution(avg:0.28/max:0.93/min:0.01)\n",
      "dimension 50, half-rank, laplace done: 14597.653632164001 seconds elapsed...\n",
      "distances distribution(avg:0.27/max:1.53/min:0.01)\n",
      "dimension 50, low-rank, laplace done: 15221.810928344727 seconds elapsed...\n",
      "distances distribution(avg:0.31/max:0.50/min:0.18)\n",
      "dimension 20, full-rank, gaussian done: 16227.329634428024 seconds elapsed...\n",
      "distances distribution(avg:0.31/max:0.56/min:0.12)\n",
      "dimension 20, half-rank, gaussian done: 16972.49710059166 seconds elapsed...\n",
      "distances distribution(avg:0.29/max:0.96/min:0.01)\n",
      "dimension 20, low-rank, gaussian done: 17514.202355146408 seconds elapsed...\n",
      "distances distribution(avg:0.28/max:1.39/min:0.09)\n",
      "dimension 20, full-rank, t done: 17959.84362435341 seconds elapsed...\n",
      "distances distribution(avg:0.27/max:3.08/min:0.05)\n",
      "dimension 20, half-rank, t done: 18442.35973572731 seconds elapsed...\n",
      "distances distribution(avg:0.24/max:1.53/min:0.01)\n",
      "dimension 20, low-rank, t done: 18995.883638381958 seconds elapsed...\n",
      "distances distribution(avg:0.28/max:0.97/min:0.01)\n",
      "dimension 20, full-rank, laplace done: 19284.05200767517 seconds elapsed...\n",
      "distances distribution(avg:0.27/max:0.90/min:0.01)\n",
      "dimension 20, half-rank, laplace done: 19569.423753976822 seconds elapsed...\n",
      "distances distribution(avg:0.25/max:1.60/min:0.00)\n",
      "dimension 20, low-rank, laplace done: 20040.94346356392 seconds elapsed...\n",
      "distances distribution(avg:0.31/max:0.43/min:0.21)\n",
      "dimension 50, full-rank, gaussian done: 22723.6844227314 seconds elapsed...\n",
      "distances distribution(avg:0.31/max:0.46/min:0.18)\n",
      "dimension 50, half-rank, gaussian done: 24701.25887775421 seconds elapsed...\n",
      "distances distribution(avg:0.31/max:0.65/min:0.05)\n",
      "dimension 50, low-rank, gaussian done: 25876.594262838364 seconds elapsed...\n",
      "distances distribution(avg:0.28/max:1.25/min:0.10)\n",
      "dimension 50, full-rank, t done: 26732.93375992775 seconds elapsed...\n",
      "distances distribution(avg:0.28/max:1.48/min:0.08)\n",
      "dimension 50, half-rank, t done: 27643.65457034111 seconds elapsed...\n",
      "distances distribution(avg:0.26/max:1.00/min:0.04)\n",
      "dimension 50, low-rank, t done: 28719.86984872818 seconds elapsed...\n",
      "distances distribution(avg:0.28/max:0.95/min:0.00)\n",
      "dimension 50, full-rank, laplace done: 29420.769441604614 seconds elapsed...\n",
      "distances distribution(avg:0.28/max:1.15/min:0.00)\n",
      "dimension 50, half-rank, laplace done: 30091.92984843254 seconds elapsed...\n",
      "distances distribution(avg:0.27/max:1.31/min:0.00)\n",
      "dimension 50, low-rank, laplace done: 30814.52835035324 seconds elapsed...\n",
      "distances distribution(avg:0.31/max:0.49/min:0.19)\n",
      "dimension 20, full-rank, gaussian done: 32003.006463766098 seconds elapsed...\n",
      "distances distribution(avg:0.31/max:0.57/min:0.13)\n",
      "dimension 20, half-rank, gaussian done: 32828.472526073456 seconds elapsed...\n",
      "distances distribution(avg:0.28/max:0.87/min:0.01)\n",
      "dimension 20, low-rank, gaussian done: 33504.114248752594 seconds elapsed...\n",
      "distances distribution(avg:0.27/max:1.46/min:0.08)\n",
      "dimension 20, full-rank, t done: 34021.67711019516 seconds elapsed...\n",
      "distances distribution(avg:0.28/max:1.94/min:0.07)\n",
      "dimension 20, half-rank, t done: 34539.320240974426 seconds elapsed...\n",
      "distances distribution(avg:0.25/max:2.34/min:0.01)\n",
      "dimension 20, low-rank, t done: 35163.39450263977 seconds elapsed...\n",
      "distances distribution(avg:0.28/max:0.96/min:0.01)\n",
      "dimension 20, full-rank, laplace done: 35503.77907156944 seconds elapsed...\n",
      "distances distribution(avg:0.27/max:1.27/min:0.00)\n",
      "dimension 20, half-rank, laplace done: 35812.892318964005 seconds elapsed...\n",
      "distances distribution(avg:0.25/max:1.49/min:0.00)\n",
      "dimension 20, low-rank, laplace done: 36307.04656648636 seconds elapsed...\n",
      "distances distribution(avg:0.31/max:0.43/min:0.24)\n",
      "dimension 50, full-rank, gaussian done: 39171.77070736885 seconds elapsed...\n",
      "distances distribution(avg:0.31/max:0.49/min:0.19)\n",
      "dimension 50, half-rank, gaussian done: 41578.77456521988 seconds elapsed...\n",
      "distances distribution(avg:0.30/max:0.57/min:0.07)\n",
      "dimension 50, low-rank, gaussian done: 42869.07462501526 seconds elapsed...\n",
      "distances distribution(avg:0.28/max:2.77/min:0.09)\n",
      "dimension 50, full-rank, t done: 43763.739505529404 seconds elapsed...\n",
      "distances distribution(avg:0.28/max:1.81/min:0.08)\n",
      "dimension 50, half-rank, t done: 44723.50453352928 seconds elapsed...\n",
      "distances distribution(avg:0.25/max:1.58/min:0.03)\n",
      "dimension 50, low-rank, t done: 45841.84302306175 seconds elapsed...\n",
      "distances distribution(avg:0.28/max:0.89/min:0.01)\n",
      "dimension 50, full-rank, laplace done: 46556.27716732025 seconds elapsed...\n",
      "distances distribution(avg:0.27/max:0.95/min:0.01)\n",
      "dimension 50, half-rank, laplace done: 47214.268996953964 seconds elapsed...\n",
      "distances distribution(avg:0.26/max:1.05/min:0.01)\n",
      "dimension 50, low-rank, laplace done: 47914.85294699669 seconds elapsed...\n",
      "distances distribution(avg:0.31/max:0.45/min:0.18)\n",
      "dimension 20, full-rank, gaussian done: 49050.70352649689 seconds elapsed...\n",
      "distances distribution(avg:0.31/max:0.57/min:0.09)\n",
      "dimension 20, half-rank, gaussian done: 49921.853222846985 seconds elapsed...\n",
      "distances distribution(avg:0.29/max:0.95/min:0.02)\n",
      "dimension 20, low-rank, gaussian done: 50581.932133197784 seconds elapsed...\n",
      "distances distribution(avg:0.29/max:3.40/min:0.06)\n",
      "dimension 20, full-rank, t done: 51090.68212223053 seconds elapsed...\n",
      "distances distribution(avg:0.27/max:4.22/min:0.05)\n",
      "dimension 20, half-rank, t done: 51598.91720342636 seconds elapsed...\n",
      "distances distribution(avg:0.25/max:2.04/min:0.01)\n",
      "dimension 20, low-rank, t done: 52236.88105964661 seconds elapsed...\n",
      "distances distribution(avg:0.27/max:1.03/min:0.01)\n",
      "dimension 20, full-rank, laplace done: 52563.82758259773 seconds elapsed...\n",
      "distances distribution(avg:0.28/max:1.29/min:0.00)\n",
      "dimension 20, half-rank, laplace done: 52882.04555988312 seconds elapsed...\n",
      "distances distribution(avg:0.24/max:1.60/min:0.01)\n",
      "dimension 20, low-rank, laplace done: 53411.73042488098 seconds elapsed...\n",
      "distances distribution(avg:0.31/max:0.43/min:0.21)\n",
      "dimension 50, full-rank, gaussian done: 56395.99724984169 seconds elapsed...\n",
      "distances distribution(avg:0.31/max:0.48/min:0.17)\n",
      "dimension 50, half-rank, gaussian done: 58774.439529657364 seconds elapsed...\n",
      "distances distribution(avg:0.30/max:0.64/min:0.06)\n",
      "dimension 50, low-rank, gaussian done: 60036.928417921066 seconds elapsed...\n",
      "distances distribution(avg:0.28/max:1.39/min:0.10)\n",
      "dimension 50, full-rank, t done: 60878.81164956093 seconds elapsed...\n",
      "distances distribution(avg:0.28/max:1.67/min:0.08)\n",
      "dimension 50, half-rank, t done: 61834.50406932831 seconds elapsed...\n",
      "distances distribution(avg:0.26/max:1.44/min:0.04)\n",
      "dimension 50, low-rank, t done: 62980.209612607956 seconds elapsed...\n",
      "distances distribution(avg:0.28/max:0.82/min:0.01)\n",
      "dimension 50, full-rank, laplace done: 63736.31513381004 seconds elapsed...\n",
      "distances distribution(avg:0.27/max:0.90/min:0.01)\n",
      "dimension 50, half-rank, laplace done: 64374.209787368774 seconds elapsed...\n",
      "distances distribution(avg:0.26/max:1.01/min:0.01)\n",
      "dimension 50, low-rank, laplace done: 65084.83356618881 seconds elapsed...\n",
      "distances distribution(avg:0.31/max:0.49/min:0.16)\n",
      "dimension 20, full-rank, gaussian done: 66225.14751529694 seconds elapsed...\n",
      "distances distribution(avg:0.31/max:0.53/min:0.12)\n",
      "dimension 20, half-rank, gaussian done: 67071.3273730278 seconds elapsed...\n",
      "distances distribution(avg:0.28/max:0.86/min:0.01)\n",
      "dimension 20, low-rank, gaussian done: 67730.58611249924 seconds elapsed...\n",
      "distances distribution(avg:0.27/max:1.20/min:0.06)\n",
      "dimension 20, full-rank, t done: 68233.90171122551 seconds elapsed...\n",
      "distances distribution(avg:0.27/max:1.22/min:0.06)\n",
      "dimension 20, half-rank, t done: 68753.471534729 seconds elapsed...\n",
      "distances distribution(avg:0.25/max:3.11/min:0.01)\n",
      "dimension 20, low-rank, t done: 69401.69508647919 seconds elapsed...\n",
      "distances distribution(avg:0.28/max:0.93/min:0.01)\n",
      "dimension 20, full-rank, laplace done: 69731.97439098358 seconds elapsed...\n",
      "distances distribution(avg:0.27/max:1.03/min:0.01)\n",
      "dimension 20, half-rank, laplace done: 70053.68698692322 seconds elapsed...\n",
      "distances distribution(avg:0.23/max:1.13/min:0.01)\n",
      "dimension 20, low-rank, laplace done: 70558.04989695549 seconds elapsed...\n",
      "distances distribution(avg:0.32/max:0.43/min:0.22)\n",
      "dimension 50, full-rank, gaussian done: 73398.88264584541 seconds elapsed...\n",
      "distances distribution(avg:0.31/max:0.49/min:0.17)\n",
      "dimension 50, half-rank, gaussian done: 75669.88476729393 seconds elapsed...\n",
      "distances distribution(avg:0.30/max:0.63/min:0.05)\n",
      "dimension 50, low-rank, gaussian done: 76993.62864351273 seconds elapsed...\n",
      "distances distribution(avg:0.28/max:1.49/min:0.10)\n",
      "dimension 50, full-rank, t done: 77898.7940773964 seconds elapsed...\n",
      "distances distribution(avg:0.28/max:2.58/min:0.09)\n",
      "dimension 50, half-rank, t done: 78832.8901708126 seconds elapsed...\n",
      "distances distribution(avg:0.28/max:2.31/min:0.03)\n",
      "dimension 50, low-rank, t done: 79962.64044594765 seconds elapsed...\n",
      "distances distribution(avg:0.28/max:0.96/min:0.01)\n",
      "dimension 50, full-rank, laplace done: 80678.95816850662 seconds elapsed...\n",
      "distances distribution(avg:0.28/max:0.89/min:0.01)\n",
      "dimension 50, half-rank, laplace done: 81349.62633514404 seconds elapsed...\n",
      "distances distribution(avg:0.27/max:1.19/min:0.02)\n",
      "dimension 50, low-rank, laplace done: 82086.59599471092 seconds elapsed...\n"
     ]
    }
   ],
   "source": [
    "n_rand = 10\n",
    "n_sampless = [1000] #,300,25,100]\n",
    "sampling_process_iter = 100\n",
    "\n",
    "\n",
    "pop_dist_types = ['gaussian','t','laplace']\n",
    "\n",
    "tic = time.time()\n",
    "\n",
    "dims = [2,5,10,20,50,100,500]\n",
    "for i_c in range(n_rand):\n",
    "    for n_samples in n_sampless:\n",
    "        result_compr = {}\n",
    "        for dim in dims:\n",
    "            seed = 50+dim+i_c\n",
    "            np.random.seed(seed)\n",
    "            manifold = Hyperboloid(dim)\n",
    "            result_compr[dim] = {}\n",
    "            \n",
    "            mu = np.array([1.0]+[0.0]*dim)\n",
    "                    \n",
    "            for pop_dist_type in pop_dist_types:\n",
    "                result_compr[dim][pop_dist_type] = {}\n",
    "                \n",
    "                for u,u_covs in covs[dim].items():\n",
    "                    try:\n",
    "                        res_ = riemannian_sample_variance_comparison_hyperbolic(\n",
    "                            manifold=manifold,\n",
    "                            mu=mu,\n",
    "                            cov=u_covs[0],\n",
    "                            pop_dist_type=pop_dist_type,\n",
    "                            n_samples=n_samples,\n",
    "                            sample_iter=sampling_process_iter,\n",
    "                        )\n",
    "                        result_compr[dim][pop_dist_type][u] = res_\n",
    "                        print(f'dimension {dim}, {u}, {pop_dist_type} done: {time.time()-tic} seconds elapsed...')\n",
    "                    except ValueError:\n",
    "                        print(f'[dimension {dim}, {u}, {pop_dist_type}] norm exploding error ...')\n",
    "                        \n",
    "\n",
    "        with open(f'hyperbolic/varofsamplemean_simul_n{n_samples}_cov3type_mest_{i_c}.pkl','wb') as f_:\n",
    "            pickle.dump(result_compr,f_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f31b93c",
   "metadata": {},
   "source": [
    "## different random seed test (d=10, low-rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6f22c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rand = 10\n",
    "n_sampless = [1000] #,300,25,100]\n",
    "sampling_process_iter = 100\n",
    "\n",
    "\n",
    "pop_dist_types = ['gaussian','t','laplace']\n",
    "\n",
    "tic = time.time()\n",
    "\n",
    "dims = [10]\n",
    "for i_c in range(n_rand):\n",
    "    for n_samples in n_sampless:\n",
    "        result_compr = {}\n",
    "        for dim in dims:\n",
    "            seed = 5300+dim+i_c\n",
    "            np.random.seed(seed)\n",
    "            manifold = Hyperboloid(dim)\n",
    "            result_compr[dim] = {}\n",
    "            \n",
    "            mu = np.array([1.0]+[0.0]*dim)\n",
    "                    \n",
    "            for pop_dist_type in pop_dist_types:\n",
    "                result_compr[dim][pop_dist_type] = {}\n",
    "                \n",
    "                for u,u_covs in covs[dim].items():\n",
    "                    try:\n",
    "                        res_ = riemannian_sample_variance_comparison_hyperbolic(\n",
    "                            manifold=manifold,\n",
    "                            mu=mu,\n",
    "                            cov=u_covs[0],\n",
    "                            pop_dist_type=pop_dist_type,\n",
    "                            n_samples=n_samples,\n",
    "                            sample_iter=sampling_process_iter,\n",
    "                        )\n",
    "                        result_compr[dim][pop_dist_type][u] = res_\n",
    "                        print(f'dimension {dim}, {u}, {pop_dist_type} done: {time.time()-tic} seconds elapsed...')\n",
    "                    except ValueError:\n",
    "                        print(f'[dimension {dim}, {u}, {pop_dist_type}] norm exploding error ...')\n",
    "                        \n",
    "\n",
    "        with open(f'old_rawdata/hyperbolic_3_seedtest/varofsamplemean_simul_n{n_samples}_dim10_cov3type_mest_251105_{i_c}.pkl','wb') as f_:\n",
    "            pickle.dump(result_compr,f_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
